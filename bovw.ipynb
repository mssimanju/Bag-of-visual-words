{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "from scipy.cluster.vq import kmeans\n",
    "from scipy.cluster.vq import vq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the image\n",
    "image1 = cv2.imread('./data/images/IMG_20241223_150037.jpg')\n",
    "image2 = cv2.imread('./data/images/IMG_20241223_150031.jpg')\n",
    "# Convert the training image to RGB\n",
    "training_image = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "test_image = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "# Convert the training image to gray scale\n",
    "training_gray = cv2.cvtColor(training_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Create test image by adding Scale Invariance and Rotational Invariance\n",
    "#test_image = training_image\n",
    "#test_image = cv2.pyrDown(training_image)\n",
    "#test_image = cv2.pyrDown(test_image)\n",
    "\n",
    "num_rows, num_cols = test_image.shape[:2]\n",
    "\n",
    "#rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), 30, 1)\n",
    "#test_image = cv2.warpAffine(test_image, rotation_matrix, (num_cols, num_rows))\n",
    "\n",
    "test_gray = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Display traning image and testing image\n",
    "fx, plots = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "plots[0].set_title(\"Training Image\")\n",
    "plots[0].imshow(training_image)\n",
    "\n",
    "plots[1].set_title(\"Testing Image\")\n",
    "plots[1].imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create(500)\n",
    "\n",
    "train_keypoints, train_descriptor = sift.detectAndCompute(training_gray, None)\n",
    "test_keypoints, test_descriptor = sift.detectAndCompute(test_gray, None)\n",
    "\n",
    "keypoints_without_size = np.copy(training_image)\n",
    "keypoints_with_size = np.copy(training_image)\n",
    "\n",
    "cv2.drawKeypoints(training_image, train_keypoints, keypoints_without_size, color = (0, 255, 0))\n",
    "\n",
    "cv2.drawKeypoints(training_image, train_keypoints, keypoints_with_size, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display image with and without keypoints size\n",
    "fx, plots = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "plots[0].set_title(\"Train keypoints With Size\")\n",
    "plots[0].imshow(keypoints_with_size, cmap='gray')\n",
    "\n",
    "plots[1].set_title(\"Train keypoints Without Size\")\n",
    "plots[1].imshow(keypoints_without_size, cmap='gray')\n",
    "\n",
    "# Print the number of keypoints detected in the training image\n",
    "print(\"Number of Keypoints Detected In The Training Image: \", len(train_keypoints))\n",
    "\n",
    "# Print the number of keypoints detected in the query image\n",
    "print(\"Number of Keypoints Detected In The Test Image: \", len(test_keypoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_without_size2 = np.copy(test_image)\n",
    "keypoints_with_size2 = np.copy(test_image)\n",
    "\n",
    "cv2.drawKeypoints(test_image, test_keypoints, keypoints_without_size2, color = (0, 255, 0))\n",
    "\n",
    "cv2.drawKeypoints(test_image, test_keypoints, keypoints_with_size2, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display image with and without keypoints size\n",
    "fx, plots = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "plots[0].set_title(\"Test keypoints With Size\")\n",
    "plots[0].imshow(keypoints_with_size2, cmap='gray')\n",
    "\n",
    "plots[1].set_title(\"Test keypoints Without Size\")\n",
    "plots[1].imshow(keypoints_without_size2, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Brute Force Matcher object.\n",
    "bf = cv2.BFMatcher(cv2.NORM_L1, crossCheck = True)\n",
    "\n",
    "# Perform the matching between the ORB descriptors of the training image and the test image\n",
    "matches = bf.match(train_descriptor, test_descriptor)\n",
    "\n",
    "# The matches with shorter distance are the ones we want.\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "matches = matches[0:]\n",
    "result = cv2.drawMatches(training_image, train_keypoints, test_image, test_keypoints, matches, test_gray, flags = 2)\n",
    "\n",
    "# Display the best matching points\n",
    "plt.rcParams['figure.figsize'] = [30.0, 20.0]\n",
    "plt.title('Best Matching Points')\n",
    "plt.imshow(result)\n",
    "plt.show()\n",
    "\n",
    "# Print total number of matching points between the training and query images\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "matches = flann.knnMatch(train_descriptor,test_descriptor,k=2)\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append([m])\n",
    "result2 = cv2.drawMatchesKnn(training_image, train_keypoints, test_image, test_keypoints,good,None,flags=2)\n",
    "plt.rcParams['figure.figsize'] = [30.0, 20.0]\n",
    "plt.imshow(result2)\n",
    "plt.title('Best Matching Points')\n",
    "plt.show()\n",
    "print(\"\\nNumber of Matching Keypoints Between The Training and Query Images: \", len(good))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    path=(folder + \"/\")\n",
    "    for image in os.listdir(path):\n",
    "        print(path + \"/\" + image)\n",
    "        img = cv2.imread(folder + \"/\" + image)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        images.append(img)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_train_images = load_images_from_folder('./data/images')\n",
    "bag_test_images = load_images_from_folder('./data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_train_descriptors = []\n",
    "bag_train_keypoints = []\n",
    "for image in bag_train_images:\n",
    "\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    bag_train_keypoint, bag_train_descriptor = sift.detectAndCompute(image , None)\n",
    "    bag_train_keypoints.append(bag_train_keypoint)\n",
    "    bag_train_descriptors.append(bag_train_descriptor)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_train_descriptors_array =[]\n",
    "for des in bag_train_descriptors:\n",
    "    bag_train_descriptors_array.append(np.array(des).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=cv2.drawKeypoints(bag_train_images[1], bag_train_keypoints[1], 0, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(result)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bag_train_descriptors=[]\n",
    "\n",
    "for descriptors in bag_train_descriptors_array:\n",
    "    for descriptor in descriptors:\n",
    "        all_bag_train_descriptors.append(descriptor)\n",
    "\n",
    "all_bag_train_descriptors = np.stack(all_bag_train_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bag_train_descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=400\n",
    "iters=1\n",
    "codebook,variance = kmeans(all_bag_train_descriptors,k,iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_train_descriptors = []\n",
    "for image in bag_train_images:\n",
    "\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    bag_train_keypoints, bag_train_descriptor = sift.detectAndCompute(image , None)\n",
    "    bag_train_descriptors.append(bag_train_descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_words = []\n",
    "\n",
    "for descriptor in bag_train_descriptors:\n",
    "    img_visual_words, distance = vq(descriptor, codebook)\n",
    "    visual_words.append(img_visual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visual_words[0][:5])\n",
    "len(visual_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vectors = []\n",
    "for img_visual_words in visual_words:\n",
    "    # create a frequency vector for each image\n",
    "    img_frequency_vector = np.zeros(k)\n",
    "    for word in img_visual_words:\n",
    "        img_frequency_vector[word] += 1\n",
    "    frequency_vectors.append(img_frequency_vector)\n",
    "# stack together in numpy array\n",
    "frequency_vectors = np.stack(frequency_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [84,  22,  45, 172]:\n",
    "    print(f\"{i}: {frequency_vectors[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vectors[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the frequency vector for image 0\n",
    "plt.bar(list(range(k)), frequency_vectors[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=33\n",
    "df = np.sum(frequency_vectors > 0, axis=0)\n",
    "df.shape\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(n/ df)\n",
    "idf.shape\n",
    "idf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = frequency_vectors * idf\n",
    "tfidf.shape, tfidf[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(range(k)), tfidf[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "top_k = 5\n",
    "i = 16\n",
    "\n",
    "# get search image vector\n",
    "a = tfidf[i]\n",
    "b = tfidf  # set search space to the full sample\n",
    "# get the cosine distance for the search image `a`\n",
    "cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))\n",
    "# get the top k indices for most similar vecs\n",
    "idx = np.argsort(-cosine_similarity)[:top_k]\n",
    "# display the results\n",
    "for i in idx:\n",
    "    print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
    "    plt.imshow(bag_train_images[i], cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
